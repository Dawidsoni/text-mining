{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse\n",
    "import sys\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"article_finder\")\n",
    "import articles\n",
    "import embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_words_embeddings():\n",
    "    with open(\"article_finder/data/wiki_embeddings_300.txt\") as file_stream:\n",
    "        lines = [x.strip() for x in file_stream.readlines()]\n",
    "        return {x.split()[0].lower(): np.fromstring(\" \".join(x.split()[1:]), dtype=float, sep=\" \") for x in lines[1:]}\n",
    "    \n",
    "\n",
    "words_embeddings = read_words_embeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Articles terms array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_base_forms = embeddings.read_words_base_forms(path=\"article_finder/data/base_forms.txt\")\n",
    "wiki_articles = articles.read_wiki_articles(path=\"article_finder/data/wiki_slice.txt\")\n",
    "rows_article_ids = dict(enumerate([x.id for x in sorted(wiki_articles, key=lambda x: x.id)]))\n",
    "article_ids_rows = dict([(y, x) for x, y in rows_article_ids.items()])\n",
    "titles_wiki_articles = {wiki_article.title: wiki_article for wiki_article in wiki_articles}\n",
    "ids_wiki_articles = {wiki_article.id: wiki_article for wiki_article in wiki_articles}\n",
    "terms_indexes = embeddings.get_terms_indexes(words_base_forms, wiki_articles, add_missing_forms=True)\n",
    "indexes_terms = {x_y[1]: x_y[0] for x_y in terms_indexes.items()}\n",
    "idf_vector = embeddings.create_idf_vector(words_base_forms, wiki_articles, terms_indexes)\n",
    "terms_idf_values = {indexes_terms[i]: idf_vector[i] for i in range(idf_vector.shape[0])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "banned_terms = {indexes_terms[index] for index in np.argsort(idf_vector.reshape(-1, ))[:100]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOP_TERMS_COUNT = 20\n",
    "articles_terms_array = (-1) * np.ones((len(wiki_articles), TOP_TERMS_COUNT), dtype=np.int32)\n",
    "prefix_terms_indexes = {}\n",
    "for wiki_article in wiki_articles:\n",
    "    article_terms = set()\n",
    "    for word in wiki_article.content.split(\" \"):\n",
    "        for term in words_base_forms[word]:\n",
    "            if len(article_terms) >= TOP_TERMS_COUNT:\n",
    "                break\n",
    "            if term not in terms_indexes or term in banned_terms or term in article_terms or term not in words_embeddings:\n",
    "                continue\n",
    "            if term not in prefix_terms_indexes:\n",
    "                prefix_terms_indexes[term] = len(prefix_terms_indexes)\n",
    "            articles_terms_array[article_ids_rows[wiki_article.id], len(article_terms)] = prefix_terms_indexes[term]\n",
    "            article_terms.add(term)\n",
    "            \n",
    "            \n",
    "prefix_indexes_terms = {y: x for (x, y) in prefix_terms_indexes.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_terms = set(articles_terms_array[article_ids_rows[3097]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C++\n",
      "Python\n",
      "Język wysokiego poziomu\n",
      "Terminator (programowanie)\n",
      "CodeMirror\n",
      "Ymacs\n",
      "Lisp-maszyna\n"
     ]
    }
   ],
   "source": [
    "for index, row in enumerate(articles_terms_array):\n",
    "    if len(set(row).intersection(article_terms)) >= 7:\n",
    "        print(ids_wiki_articles[rows_article_ids[index]].title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WikiArticle(id=3097, title='Python', content='Python\\nPython – język programowania wysokiego poziomu ogólnego przeznaczenia , o rozbudowanym pakiecie bibliotek standardowych , którego ideą przewodnią jest czytelność i klarowność kodu źródłowego . Jego składnia cechuje się przejrzystością i zwięzłością .\\nPython wspiera różne paradygmaty programowania : obiektowy , imperatywny oraz w mniejszym stopniu funkcyjny . Posiada w pełni dynamiczny system typów i automatyczne zarządzanie pamięcią , będąc w tym podobnym do języków Perl , Ruby , Scheme czy Tcl . Podobnie jak inne języki dynamiczne jest często używany jako język skryptowy . Interpretery Pythona są dostępne na wiele systemów operacyjnych .')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titles_wiki_articles[\"Python\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating articles embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_embeddings = []\n",
    "for index, row in enumerate(articles_terms_array):\n",
    "    if any([row[i] == -1 for i in range(len(row))]):\n",
    "        rows_embeddings.append(np.zeros(300))   \n",
    "        continue\n",
    "    article_terms = [prefix_indexes_terms[row[i]] for i in range(len(row))]\n",
    "    article_embeddings = []\n",
    "    for index, term in enumerate(article_terms):\n",
    "        term_embedding = words_embeddings[term] / np.linalg.norm(words_embeddings[term])\n",
    "        weight = np.exp(-0.1 * index) \n",
    "        article_embeddings.append(weight * term_embedding)\n",
    "    rows_embeddings.append(np.mean(article_embeddings, axis=0))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.sparse.save_npz(\n",
    "    \"article_finder/data/dense_embeddings_300.npz\",\n",
    "    scipy.sparse.csr_matrix(np.array(rows_embeddings))\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
