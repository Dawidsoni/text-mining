{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import itertools\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import defaultdict, Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_word(word):\n",
    "    regex_match = re.search(\"([a-zA-Z]+)\", word)\n",
    "    if regex_match is None:\n",
    "        return None\n",
    "    return regex_match.group(1)\n",
    "\n",
    "\n",
    "def read_one_grams():\n",
    "    with open(\"1grams.txt\", \"r\") as file_stream:\n",
    "        one_grams = [clean_word(x.strip().split(\" \")[1]) for x in file_stream.readlines()]\n",
    "        return [x for x in one_grams if x is not None and len(x) > 0]\n",
    "    \n",
    "\n",
    "def read_words_base_forms():\n",
    "    words_base_forms = defaultdict(lambda: [])\n",
    "    with open(\"base_forms.txt\", \"r\") as fstream:\n",
    "        bases_words = map(lambda x: x.split(\";\")[0:2], fstream.readlines())\n",
    "        for base, word in bases_words:\n",
    "            words_base_forms[word].append(base)\n",
    "    return dict(words_base_forms)\n",
    "\n",
    "\n",
    "def fetch_correction_words(word):\n",
    "    cleaned_words = [clean_word(x) for x in word.split(\".\")]\n",
    "    if len(cleaned_words) == 1:\n",
    "        return []\n",
    "    return [word for word in cleaned_words if word is not None and len(word) > 0]\n",
    "\n",
    "\n",
    "def read_two_grams_with_correction_words():\n",
    "    with open(\"2grams.txt\", \"r\") as file_stream:\n",
    "        dirty_two_grams = [tuple(x.strip().split(\" \")[1:3]) for x in file_stream.readlines()]\n",
    "        cleaned_two_grams = []\n",
    "        correction_words = []\n",
    "        for two_gram in dirty_two_grams:\n",
    "            cleaned_word1, cleaned_word2 = clean_word(two_gram[0]), clean_word(two_gram[1])\n",
    "            if cleaned_word1 is None or cleaned_word2 is None:\n",
    "                continue\n",
    "            if len(cleaned_word1) == 0 or len(cleaned_word2) == 0:\n",
    "                continue\n",
    "            cleaned_two_grams.append((cleaned_word1, cleaned_word2))\n",
    "            correction_words.extend(fetch_correction_words(two_gram[0]))\n",
    "            correction_words.extend(fetch_correction_words(two_gram[1]))\n",
    "    return cleaned_two_grams, correction_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_grams_counter = Counter(read_one_grams())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_grams, correction_words = read_two_grams_with_correction_words()\n",
    "two_grams_counter = Counter(two_grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_base_forms = read_words_base_forms()\n",
    "words = set(words_base_forms.keys())\n",
    "base_forms = set(itertools.chain(*words_base_forms.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_merged_words():\n",
    "    merged_words = []\n",
    "    for one_gram in set(one_grams_counter.keys()).union(correction_words):\n",
    "        for split_index in range(1, len(one_gram)):\n",
    "            word1, word2 = one_gram[:split_index], one_gram[split_index:]\n",
    "            if word1 not in words or word2 not in words:\n",
    "                continue\n",
    "            one_gram_count = one_grams_counter[one_gram]\n",
    "            if len(word1) <= 3 or len(word2) <= 3:\n",
    "                continue\n",
    "            if two_grams_counter[(word1, word2)] >= one_gram_count * 0.5 and one_gram_count > 0:\n",
    "                merged_words.append((word1, word2))\n",
    "                continue\n",
    "            count1, count2 = one_grams_counter[word1], one_grams_counter[word2]\n",
    "            if min(count1, count2) <= 20:\n",
    "                continue\n",
    "            count_diff = abs(count1 - count2)         \n",
    "            if count_diff <= min(count1, count2) * 0.3 and sum([count1, count2]) * 0.01 >= one_gram_count:\n",
    "                merged_words.append((word1, word2))                \n",
    "    return merged_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7246"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_words = find_merged_words()\n",
    "len(merged_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[True, True, True, True]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EXPECTED_MERGED_WORDS = [\n",
    "    (\"wielkiego\", \"pomorska\"),\n",
    "    (\"socjologii\", \"uniwersytetu\"),\n",
    "    (\"otwarta\", \"pracownia\"),\n",
    "    (\"przez\", \"grzechy\"),\n",
    "]\n",
    "[x in merged_words for x in EXPECTED_MERGED_WORDS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[True, True, True, True]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "UNEXPECTED_MERGED_WORDS = [\n",
    "    (\"anty\", \"systemową\"),\n",
    "    (\"super\", \"tygrysa\"),\n",
    "    (\"wewnątrz\", \"oddziałowego\"),\n",
    "    (\"wschodnio\", \"karpackiego\"),\n",
    "]\n",
    "[x not in merged_words for x in UNEXPECTED_MERGED_WORDS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(146, 39, 3, 2)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_grams_counter['agro'], one_grams_counter['grzechy'], two_grams_counter[('glaxo', 'smith')], one_grams_counter['przezgrzechy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"agro\" in words_base_forms.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('mama', 'tata'),\n",
       " ('ponad', 'stuosobowy'),\n",
       " ('spam', 'killer'),\n",
       " ('biuro', 'literackie'),\n",
       " ('propagandowy', 'gest'),\n",
       " ('pycha', 'tego'),\n",
       " ('nike', 'owcy'),\n",
       " ('jest', 'patrze'),\n",
       " ('kamer', 'gier'),\n",
       " ('prze', 'trwaj')]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[merged_words[i] for i in np.random.choice(len(merged_words), 10, replace=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_split_words():\n",
    "    split_words = []\n",
    "    for word1, word2 in two_grams_counter.keys():\n",
    "        merged_word = word1 + word2\n",
    "        one_gram_count = one_grams_counter[merged_word]\n",
    "        if len(word1) <= 3 or len(word2) <= 3:\n",
    "            continue\n",
    "        if two_grams_counter[(word1, word2)] < one_gram_count * 0.4 and one_gram_count > 0:\n",
    "            split_words.append(merged_word)\n",
    "    return split_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5500"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_words = find_split_words()\n",
    "len(split_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['egotycznie',\n",
       " 'freiburg',\n",
       " 'zakupowych',\n",
       " 'nowoczesne',\n",
       " 'powodowi',\n",
       " 'cudzoziemskiej',\n",
       " 'produkowano',\n",
       " 'sprawiedliwo',\n",
       " 'wysokospecjalistyczna',\n",
       " 'hitlerowskie']"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[split_words[i] for i in np.random.choice(len(split_words), 10, replace=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
